\section{Implications for State of the Practice}
\label{sec:findings}

We found that the combination of systemwide telemetry, active performance
probes, modular data integration, and generalized analysis
tools was highly effective in deriving insight from otherwise
opaque large-scale storage systems.  Over the course of this study we
codified successful techniques, such as adaptations of financial analysis
strategies, into our open source \tokio framework
(see Appendix \ref{sec:appendix/artifacts}) and contribute to the state of
the practice by making this framework
readily available so that the methodology is repeatable on other platforms.  Our framework notably
decouples analysis techniques from data integration so that the
the same analysis tools can be reused at any facility once modular
connectors have been added to normalize that facility's telemetry data.

We also contribute to the state of the practice by uncovering novel
insights into the nature of I/O performance in production storage systems.
In the remainder of this section, we revisit the most significant outcomes and
provide corollaries that improve the ability for HPC storage practitioners
to contextualize, quantify, and analyze I/O performance variability on
large-scale storage systems.

\subsection{Understanding large-scale storage system behavior}

The following findings  refine our understanding of how large-scale storage
systems behave at a high level. 

\begin{itemize}[leftmargin=*]

\item \textbf{Baseline I/O performance and variability are not constant over
time:}
This observation has direct implications for our specifications and expectations of system performance.
It is unrealistic to assume that benchmarks performed upon system delivery
will accurately represent performance over time, and performance
expectations must be recalibrated as storage systems age.  We also note that
our analysis was not able to account for all variability; additional work is
needed in both analysis techniques and monitoring.

\item \textbf{Attributes that correlate with transient performance problems often differ from those that correlate with long-term performance problems:}
I/O-intensive workloads in HPC are widely known to be unusually bursty compared with other I/O-intensive computing workloads, but this study demonstrates that some aspects of HPC I/O workloads (such as IOPS and metadata operations) are more bursty than others (such as bandwidth utilization).
Performance degradation that results from IOPS or metadata contention is unlikely to persist for days, whereas bandwidth contention can result in performance degradation at all time scales.

\end{itemize}


\subsection{Improving monitoring and telemetric coverage}

The following findings motivate further advances in how large-scale storage systems are monitored and what data sources are required.

% \TODO{\textbf{where does this go?  tempted to drop it since it's specific to our results, not the state of the practice}
% Furthermore, 16\% of the transient I/O performance issues defied classification using our binary classification method.
% This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation.}

\begin{itemize}[leftmargin=*]

\item \textbf{Administrative activities such as system patches and updates are a significant source of time-dependent, long-term performance variation}:
HPC systems are complex, and their upgrades may be dictated by external factors including maintenance schedules, vendor release cycles, and security disclosures.
Thus, capturing explicit measurements before and after the upgrade process may not be possible.
Continuous monitoring and active probing of performance mitigate this
problem by making such measurements a routine procedure regardless of
upgrade schedule, much in the same way that continuous integration testing automatically monitors software development processes.
Every large-scale facility incorporates testing procedures into its upgrade
strategy, but this study highlights the need for breadth of performance
testing across workload motifs.

\item \textbf{Holistic I/O monitoring should incorporate environmental
provenance information such as kernel, OS, and file system versions}: 
This is an obvious finding in retrospect but is not widely taken into
account in current instrumentation tools.
Tools such as Darshan and LMT should capture sufficient environmental information alongside conventional performance measurements 
to aid in correlation between performance losses and environmental changes.

\item \textbf{Bandwidth contention from sustained, I/O-intensive workloads often accompanies sustained performance losses}:
The impact of bandwidth contention on I/O performance is widely supported in the literature, but this study demonstrates time-dependent behavior not previously measured.
Detrimental contention can occur over time spans lasting several weeks (e.g., aggregate workload due to project allocation timing) and may be driven by factors 
not captured by conventional HPC monitoring (e.g., wide area transfers or archival traffic).
This calls for a broadening of the definition of ``holistic I/O characterization'' to include not just the full HPC I/O stack but also the auxiliary resources that utilize the storage system.

\end{itemize}


\subsection{Analyzing evolving storage systems}

The following findings highlight ways to improve the accuracy and significance of I/O analysis and modeling.

\begin{itemize}[leftmargin=*]

\item \textbf{Significantly stronger correlations can be found by focusing analysis on systematically identified regions of interest in the data}:
With the proliferation of machine learning, it is tempting to apply unguided techniques to large data sets to extract insights.
However, we have demonstrated that a systems practitioner will gain more confident insights from targeted analysis of relevant regions of interest.
The region identification method need not depend on SMAs, and alternative approaches for both partitioning time series data and classifying the measurements can be easily replaced.
The simple technique we applied here still identified causes of poor performance in 84\% of the jobs that experienced transient performance loss.
Applying more sophisticated classification methods is likely to improve upon this.

%Our hope is that the simple statistical methods presented here will advance
%the state of the practice across the HPC community by encouraging
%straightforward methods that improve the efficacy of quantitative I/O analysis.

\item \textbf{The nature and magnitude of how different attributes correlate with I/O performance also change over time:}
This observation has critical ramifications for developing I/O performance models.
Models developed from a training set without temporal context will produce incorrect predictions, even on the same target system, if
external factors have caused the performance of that system to evolve.
For example, high CPU load was found to correlate with favorable performance under healthy file system conditions, yet it also coincides with unfavorable performance when non-I/O workloads are impacting storage servers.

\item \textbf{Financial market analysis techniques can be adapted to I/O performance time series data to attenuate noise and identify underlying trends}:
Processing a large, continuously expanding, and noisy time series data set is not unique to I/O performance characterization.
We found similarities between our data set and financial market data that enabled straightforward adaptation of known techniques and terminology.
More advanced market analysis strategies or strategies from other fields would likely be more effective.

\end{itemize}
