\section{Findings and Implications for State of the Practice}
\label{sec:findings}

% The key findings of this study have been presented thus far in an
% order dictated by a hierarchical, data-driven investigation of our
% I/O performance data set.  We can now revisit those findings, however,
% and reclassify them according to their impact the state of
% the practice.

This study has revealed a number of novel insights into the nature of I/O performance in production using holistic I/O analysis and active I/O performance probes.
In addition, our findings have significance beyond the scope of the specific systems evaluated, and they collectively constitute an advancement to the greater state of the practice.
In this section, we revisit the most significant outcomes and provide corollaries that improve the ability to contextualize, quantify, and analyze I/O performance variability on large-scale storage systems.

\subsection{Understanding large-scale storage system behavior}

The following findings serve to refine our understanding of how large-scale storage
systems behave at a high level: 

\begin{itemize}[leftmargin=*]

\item \textbf{Baseline I/O performance and variability are not constant over
time:}
This observation has direct implications for our specifications and expectations of system performance.
It is unrealistic to assume that benchmarks performed upon system delivery will accurately represent performance over time, and performance expectations must be re-calibrated as storage systems age.

\item \textbf{Attributes that correlate with transient performance problems often differ from those that correlate with long-term performance problems:}
I/O-intensive workloads in HPC are widely known to be unusually bursty compared to other I/O-intensive computing workloads, but this finding reveals that some aspects of HPC I/O workloads (such as IOPS and metadata contention) are burstier than others (such as bandwidth contention).
Performance degradation that results from metadata or IOPS contention is unlikely to persist for days, while bandwidth contention can result in performance degradation at all time scales.

\end{itemize}


\subsection{Improving monitoring and telemetric coverage}

The following findings motivate further advances in how large-scale storage systems are monitored and what data sources are required:

% \TODO{\textbf{where does this go?  tempted to drop it since it's specific to our results, not the state of the practice}
% Furthermore, 16\% of the transient I/O performance issues defied classification using our binary classification method.
% This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation.}

\begin{itemize}[leftmargin=*]

\item \textbf{Administrative activities such as maintenance patches and
software updates are a significant source of time-dependent, long term
performance variation}:
HPC systems are complex, and their upgrades may be dictated by a variety of external factors including maintenance schedules, vendor release cycles, and security concerns.
Because of this, it may not be possible to capture explicit before and after measurements as part of the upgrade process.
Continuous monitoring and active probing of performance can mitigate this problem by making such measurements a routine procedure regardless of upgrade schedule, much in the same way that continuous integration testing is used to automatically monitor software development processes.

\item \textbf{Holistic I/O monitoring should incorporate environmental
provenance information, such as kernel, operating system, and file system versions, to aid in correlation}: 
This is an obvious finding in retrospect, but is not widely taken into
account in current instrumentation tools.
Tools such as Darshan and LMT should capture sufficient environmental information alongside conventional performance measurements to
simplify the task of identifying performance losses due to environmental changes.

\item \textbf{Bandwidth contention from sustained, I/O-intensive workloads often accompanies sustained performance losses}:
The impact of bandwidth contention on I/O performance is widely supported in the literature, but this study demonstrates time-dependent behavior not previously measured.
Specifically, detrimental contention
can occur over time spans lasting several weeks (e.g., aggregate
workload due to project allocation timing) and may be driven by factors 
not captured by conventional HPC monitoring (e.g., wide area transfers or archival traffic).
This calls for a broadening of the definition of ``holistic I/O characterization'' to include not just the full HPC I/O stack, but also the auxiliary resources that utilize the storage system.

\end{itemize}


\subsection{Analyzing evolving storage systems}

Finally, the following findings highlight ways in which the accuracy and significance of I/O analysis and modeling methodologies can be improved:

\begin{itemize}[leftmargin=*]

\item \textbf{Significantly stronger correlations can be found by focusing analysis on algorithmically identified regions of interest in the data}:
With the emergence of machine learning, i to apply unguided techniques to a large data set in a bid to extract meaning from it.
From a practical point of view, however, a systems practitioner will gain more benefit from targeted analysis of relevant regions of interest.
The region identification method need not depend on SMAs, and alternative approaches for both partitioning time series data and classifying the measurements within regions can be easily improved with more sophisticated methods.
The simple method we applied here was able to classify 84\% of jobs that suffered from transient performance loss.

%Our hope is that the simple statistical methods presented here will advance
%the state of the practice across the HPC community by encouraging
%straightforward methods that improve the efficacy of quantitative I/O analysis.

\item \textbf{The nature and magnitude of how different attributes correlate with I/O performance also change over time:}
This observation has critical ramifications for developing and applying I/O performance models.
Models developed from a training set without temporal context will produce incorrect predictions, even on the same target system, if
external factors have caused the performance of that system to evolve over time.
For example, we demonstrated that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.

\item \textbf{Financial market technical analysis techniques can be adapted to I/O performance time series data to attenuate noise and identify underlying trends}:
Processing a large, continuously expanding, and noisy time series data set is a daunting task, but one that is not unique to I/O performance characterization.
We found similarities between our data set and financial market data that enabled straightforward adaptation of known techniques and terminology.
More advanced financial market technical analysis strategies or strategies from other similar fields would likely be even more effective.

\end{itemize}