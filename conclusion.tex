\section{Findings and Implications for State of the Practice}
\label{sec:findings}

The order in which findings were discovered in our study was largely
dictated by the logical order of how the data was gradually explored in
greater resolution.  We can now revisit those findings, however, and
classify them according to their ramifications on the state of the practice.  

The following findings motivate further advances in the characterization of
large-scale storage systems:

\begin{itemize}

\item \textbf{Administrative activities such as maintenance patches and
software updates are a significant source of time-dependent, long term
performance variation}: HPC systems are complex, and their upgrades may be
dictated by a variety of external factors including maintenance schedules,
vendor release cycles, and security concerns.  Because of this, it is often
implausible to capture explicit before and after measurements as part of the
upgrade process.  Continuous monitoring and active probing of performance
can mitigate this by making such measurements routine regardless of upgrade
schedule, much in the same way that continuous integration  testing is used
to automatically monitor software development processes.

\item \textbf{Holistic I/O monitoring should incorporate environmental
provenance information, such as kernel, operating system, and file system
versions, to aid in correlation}: 
In retrospect this is an obvious finding, but is not widely taken into
account in current instrumentation tools.  Tools such as Darshan and
TOKIO should capture some degree of environmental information (kernel
versions etc.) to store alongside performance measurements in order to
simplify the task of aligning those pieces of information.

\item \textbf{Bandwidth contention from sustained workloads often accompany
sustained performance losses}: The impact of bandwidth contention on I/O
performance is widely supported in the literature.  This study highlights
subtleties in that observation however; namely that detremental contention
can occur time spans lasting as much as several weeks (e.g., aggregate
workload arising from project allocation timing) and may be driven by factors 
not capture by conventional HPC monitoring (e.g., wide area transfer or 
archival traffic).  This calls for a broadening of the definition of
``holistic I/O characterization'' to include not just the full HPC I/O
stack, but also the auxiliary resources that utilize the storage system.

\end{itemize}

The following findings refine our high-level understanding of how
large-scale storage systems behave:

\begin{itemize}

\item \textbf{Baseline I/O performance and variability are not constant over
time}: This observation has direct impliciations for specification and
expectations of performance.  It is unrealistic to benchmark a new storage
system upon delivery and assume constant performance from that point on.

\item \textbf{Attributes that correlate with transient performance problems (IOPS
and metadata contention) often differ from those that correlate with
long-term performance problems (bandwidth contention).} HPC I/O workloads
are widely known to be unusually bursty compared to other types of storage
system. This finding suggests that this can be further refined to indicate
that some aspects of I/O activity are burstier than others, and that those
will affect performance in different ways.  We see distinctly different
trends and correlations for sustained bandwidth contention than we do for
IOPs and metadata contention.

\end{itemize}

Finally, the following findings indicate ways in which we can improve
I/O analysis and modeling methodology:

\begin{itemize}

\item \textbf{Financial market technical analysis techniques can be adapted
to time series I/O performance data to attenuate noise and identify
underlying trends}: Processing a large, continuously expanding, and noisy
time series data set is a daunting task, but one that is not unique to I/O
performance capture by any means.  In our study we found similarities
between our data set and financial market data that enabled straightforward
adaptation of known techniques and terminology.  More advanced financial
market technical analysis strategies would likely be even more effective.

\item \textbf{Significantly stronger correlations can be found by focusing
analysis on algorithmically identified regions of interest in the data}:
There is considerable temptation with the emergence of machine learning and
deep learning to apply unguided techniques to a large data set in a bid to
extract meaning from it. From a practical point of view, however, a systems
practitioner will get more benefit from focused analyis of urgent regions of
interest.  Such regions can be identified algorithmicly with simple
analytical techniques.

\item \textbf{The nature and magnitude of how different attributes correlate
with I/O performance also change over time}  This observation has critical
ramifications for I/O modeling techniques.  A model derived from a training data set
will produce incorrect predictions, even on the same target system, if
external factors have caused the performance of that system to evolve over
time. 

\end{itemize}

\section{Conclusion} \label{sec:conclusions}

\TODO{Go through this, see if anything specific was not captured above, zoom
out remainder into higher level conclusions and future work.}

% 
% - baseline I/O performance and variability are not constant over time
% 
% - administrative activities such as maintenance patches and software updates are a significant source of time-dependent, long term performance variation
%
% - holistic I/O monitoring should incorporate environmental provenance information, such as kernel, operating system, and file system versions, to aid in correlation
% 
% - financial market technical analysis techniques can be adapted to time series I/O performance data to attenuate noise and identify underlying trends
%
% - Significantly stronger correlations can be found by focusing analysis on algorithmically identified regions of interest in the data.
%
% - bandwidth contention from sustained workloads often accompany sustained performance losses
%
% - the nature and magnitude of how different attributes correlate with I/O performance also change over time
% 
% - attributes that correlate with transient performance problems (IOPS and metadata contention) often differ from those that correlate with long-term performance problems (bandwidth contention).
% 

In this work we have gained numerous insights into I/O performance variation
in production on three leadership-class HPC resources. By collecting
performance data for one-year we were able observe both short-term and
long-term effects upon I/O performance. We also developed a set of best
practices for others interested in monitoring and understanding the performance of their HPC I/O systems in production.

Our results show that the baseline I/O performance changes over time. The
baseline peak performance for a given I/O motif on a file systems can change
as a result of factors such as system software updates and sustained
workloads brought on by external factors such as end of allocation year. By introducing SMA's as a mechanism of analyzing the time series data we were able to identify transitions between these regions of differing baseline I/O performance and gain insight into the cause of the performance change. 

The identification of such regions was essential to gain a clear understanding of the causes of performance changes between regions. Our results showed that the magnitude and sign of correlation between performance and other measured metrics can vary between different performance regions.
We demonstrated that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.

We showed that contention for bandwidth, IOPS, and metadata resources can be confidently determined to be the sources of transient I/O performance  problems.
That said, 16\% of the transient I/O performance issues defied classification using our binary classification method.
This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation. Addressing this need will be the subject of future work. 

The methods presented here are not dependent on SMAs, and alternative
approaches for both partitioning time series data and classifying the
measurements within regions can be replaced with more sophisticated methods.
Our hope is that the simple statistical methods presented here will advance
the state of the practice across the HPC community by encouraging
straightforward methods that improve the efficacy of quantitative I/O analysis.

In future work we plan to develop methods to mathematically classify the similarity of different regions from one another to enable the determination of broad classes of performance regions. We also plan to use the measured data as input to simulation frameworks to enable the design of potential new file system features or policies to reduce the  amount of I/O performance variation seen in production. 



\endinput

\begin{enumerate}

\TODO{these lack "implications for state of the practice"; need to take these up a level.  if we were selling this process to another center, what would they need to take from this study?}

\item \textbf{Baseline performance and performance variation changes over time.}
We have shown that the baseline peak performance for a given I/O motif on a file systems can change over time as a result of factors such as system software updates and sustained workloads motivated by external factors such as end of allocation year.
\TODO{1. you can't just measure performance on day 1}

\item \textbf{The magnitude and sign of correlation between performance and other measured metrics also varies over time.} 
We demonstrated that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.
\TODO{2. be careful how you interpret your metrics; training a model on one dataset might not make it apply to others}

\item \textbf{Bandwidth, IOPS, and metadata contention are often confidently correlated with I/O performance problems are occurring.}
We were able to obtain statistically significant trends about isolated performance transients by aggregating simple binary classifications based on coincident observations of worst values within regions.
That said, some jobs defied classification using our binary classification method.
This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation.
\TODO{3. metadata contention is burstier than bandwidth, but they both affect performance.  just at different time scales}

\item The methods presented here are not dependent on SMAs, and alternative approaches for both partitioning time series data and classifying the measurements within regions can be replaced with more sophisticated methods.
What we have shown is that a great deal of information about I/O performance variation can be quantified using holistic I/O analysis and simple statistical methods.
\end{enumerate}
