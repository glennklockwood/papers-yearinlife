\section{Findings and Implications for State of the Practice}
\label{sec:findings}

Up to this point, our findings have been highlighted in the order that
they appeared based on a data-driven exploration of the I/O performance
data set.  We can now revisit those findings, however, and classify them
according to how they impact the state of the practice.

The following findings motivate further advances in the characterization of
large-scale storage systems:

\begin{itemize}

\item \textbf{Administrative activities such as maintenance patches and
software updates are a significant source of time-dependent, long term
performance variation}: HPC systems are complex, and their upgrades may be
dictated by a variety of external factors including maintenance schedules,
vendor release cycles, and security concerns.  Because of this, it is often
implausible to capture explicit before and after measurements as part of the
upgrade process.  Continuous monitoring and active probing of performance
can mitigate this by making such measurements routine regardless of upgrade
schedule, much in the same way that continuous integration  testing is used
to automatically monitor software development processes.

\item \textbf{Holistic I/O monitoring should incorporate environmental
provenance information, such as kernel, operating system, and file system
versions, to aid in correlation}: 
In retrospect this is an obvious finding, but is not widely taken into
account in current instrumentation tools.  Tools such as Darshan and
TOKIO should capture some degree of environmental information (kernel
versions etc.) to store alongside performance measurements in order to
simplify the task of aligning those pieces of information.

\item \textbf{Bandwidth contention from sustained workloads often accompany
sustained performance losses}: The impact of bandwidth contention on I/O
performance is widely supported in the literature.  This study highlights
subtleties in that observation however; namely that detremental contention
can occur time spans lasting as much as several weeks (e.g., aggregate
workload arising from project allocation timing) and may be driven by factors 
not capture by conventional HPC monitoring (e.g., wide area transfer or 
archival traffic).  This calls for a broadening of the definition of
``holistic I/O characterization'' to include not just the full HPC I/O
stack, but also the auxiliary resources that utilize the storage system.

\end{itemize}

The following findings refine our high-level understanding of how
large-scale storage systems behave:

\begin{itemize}

\item \textbf{Baseline I/O performance and variability are not constant over
time}: This observation has direct impliciations for specification and
expectations of performance.  It is unrealistic to benchmark a new storage
system upon delivery and assume constant performance from that point on.

\item \textbf{Attributes that correlate with transient performance problems (IOPS
and metadata contention) often differ from those that correlate with
long-term performance problems (bandwidth contention).} HPC I/O workloads
are widely known to be unusually bursty compared to other types of storage
system. This finding suggests that this can be further refined to indicate
that some aspects of I/O activity are burstier than others, and that those
will affect performance in different ways.  We see distinctly different
trends and correlations for sustained bandwidth contention than we do for
IOPs and metadata contention.  Furthermore, 16\% of the transient I/O performance issues defied classification using our binary classification method.
This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation. 

\end{itemize}

Finally, the following findings indicate ways in which we can improve
I/O analysis and modeling methodology:

\begin{itemize}

\item \textbf{Financial market technical analysis techniques can be adapted
to time series I/O performance data to attenuate noise and identify
underlying trends}: Processing a large, continuously expanding, and noisy
time series data set is a daunting task, but one that is not unique to I/O
performance capture by any means.  In our study we found similarities
between our data set and financial market data that enabled straightforward
adaptation of known techniques and terminology.  More advanced financial
market technical analysis strategies would likely be even more effective.

\item \textbf{Significantly stronger correlations can be found by focusing
analysis on algorithmically identified regions of interest in the data}:
There is considerable temptation with the emergence of machine learning and
deep learning to apply unguided techniques to a large data set in a bid to
extract meaning from it. From a practical point of view, however, a systems
practitioner will get more benefit from focused analyis of urgent regions of
interest.  The region identification method need not depend on SMAs, and alternative
approaches for both partitioning time series data and classifying the
measurements within regions can be replaced with more sophisticated methods.
Our hope is that the simple statistical methods presented here will advance
the state of the practice across the HPC community by encouraging
straightforward methods that improve the efficacy of quantitative I/O analysis.

\item \textbf{The nature and magnitude of how different attributes correlate
with I/O performance also change over time}  This observation has critical
ramifications for I/O modeling techniques.  A model derived from a training data set
will produce incorrect predictions, even on the same target system, if
external factors have caused the performance of that system to evolve over
time. We demonstrated that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.

\end{itemize}

\section{Conclusion} \label{sec:conclusions}

Our study of a year in the life of a parallel file system provided a
variety of insights into performance variation on production systems,
including both long-term trends and transient anomalies.  These insights,
and the methods that we used to derive them, have implications on
instrumentation methods, administrative expectations, and analyis
techniques. Some of the recommendatations in these findings can be acted on
now, by broadening instrumentation
scope or enhancing analysis tools based on observations in this paper.
Others more fundamentally motivate the need for ``live'' analysis of
production systems, so that the lessons learned here (especially those
related to dynamic time varying behavior) can be more generally extracted
at any time from a running system to produce actionable feedback.

In future work we plan to develop methods to mathematically classify the similarity of different regions from one another to enable the determination of broad classes of performance regions. We also plan to use the measured data as input to simulation frameworks to enable the design of potential new file system features or policies to reduce the  amount of I/O performance variation seen in production. 

\endinput

\begin{enumerate}

\TODO{these lack "implications for state of the practice"; need to take these up a level.  if we were selling this process to another center, what would they need to take from this study?}

\item \textbf{Baseline performance and performance variation changes over time.}
We have shown that the baseline peak performance for a given I/O motif on a file systems can change over time as a result of factors such as system software updates and sustained workloads motivated by external factors such as end of allocation year.
\TODO{1. you can't just measure performance on day 1}

\item \textbf{The magnitude and sign of correlation between performance and other measured metrics also varies over time.} 
We demonstrated that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.
\TODO{2. be careful how you interpret your metrics; training a model on one dataset might not make it apply to others}

\item \textbf{Bandwidth, IOPS, and metadata contention are often confidently correlated with I/O performance problems are occurring.}
We were able to obtain statistically significant trends about isolated performance transients by aggregating simple binary classifications based on coincident observations of worst values within regions.
That said, some jobs defied classification using our binary classification method.
This indicates that we are still missing telemetry from important components of the I/O subsystem that contribute to performance variation.
\TODO{3. metadata contention is burstier than bandwidth, but they both affect performance.  just at different time scales}

\item The methods presented here are not dependent on SMAs, and alternative approaches for both partitioning time series data and classifying the measurements within regions can be replaced with more sophisticated methods.
What we have shown is that a great deal of information about I/O performance variation can be quantified using holistic I/O analysis and simple statistical methods.
\end{enumerate}
