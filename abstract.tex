% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
 

% I/O performance is a critical aspect of data-intensive scientific computing,
% but it is notoriously difficult to understand and diagnose. This problem
% is exacerbated by the current trends toward 
% more complex I/O architectures and more diverse applications that must
% be sustained over multiyear production life spans.

% In this work we seek to advance the state of the practice in understanding
% and diagnosing large-scale I/O performance issues. We present a
% comprehensive I/O performance data set assembled via continuous
% instrumentation methods and active performance probes across a full year of production storage activity,
% at two leadership-scale computing facilities, on five file systems,
% connected to three machines.  We then demonstrate techniques to identify
% regions of interest in large-scale I/O performance data sets, perform
% focused investigations of both long-term trends and transient anomalies,
% and uncover the contributing factors that lead to performance fluctuation.

% We find that a year in the life of a parallel file system comprises distinct regions of longer-term performance variation in addition to short-term performance transients.
% We demonstrate how systematic identification of these performance regions,
% combined with comprehensive telemetry and statistical analysis, allow us to
% isolate the factors contributing to different performance maladies at different time scales.
% From this, we present specific lessons learned and important considerations for HPC storage practitioners.

% 150 WORD VERSION HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
I/O performance is a critical aspect of data-intensive scientific computing.
We seek to advance the state of the practice in understanding and diagnosing I/O performance issues through investigation of a comprehensive I/O performance data set that captures a full year of production storage activity at two leadership-scale computing facilities.
We demonstrate techniques to identify regions of interest, perform focused investigations of both long-term trends and transient anomalies, and uncover the contributing factors that lead to performance fluctuation.

We find that a year in the life of a parallel file system is comprised of distinct regions of long-term performance variation in addition to short-term performance transients.
We demonstrate how systematic identification of these performance regions, combined with comprehensive analysis, allows us to isolate the factors contributing to different performance maladies at different time scales.
From this, we present specific lessons learned and important considerations for HPC storage practitioners.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In order to
% derive effective actionable insight in this environment, practitioners
% must apply instrumentation, analysis methods, and performance models
% that are adaptable in the face of changing conditions.

%
% In this work, we explore how algorithmic methods can be used to ease the
% burden of extracting actionable insight from long-running production I/O
% instrumentation.  We demonstrate our methodology on a full year's worth
% of telemetry data collected at two leading HPC facilities.  In doing so,
% we answer the following questions: How do we detect critical inflection
% points in performance?  How do we assess the scope and contributing
% factors for a given inflection point?  Does intuition and methodology
% derived from individual application anecdotes match the reality of
% long-term production behavior?  Can underlying trends be identified
% beyond the noise of day-to-day performance variability?
%
% Our analysis, and key case studies derived from that analysis, reveal the
% presence of both short-term and long-term performance deviations.  We also
% find that the ramifications of these performance deviations are not
% uniform
% across applications and can thus affect users in unexpected ways.

\end{abstract}
