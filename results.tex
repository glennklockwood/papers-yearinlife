\section{Investigating Trends}\label{sec:results}

SMA crossover-based partitioning of performance observations provides
a systematic method for grouping time-correlated performance events.
Once a divergence region (i.e., a performance trend) has been identified,
more focused statistical analyses can then be applied within the region
to gain insight the factors that contributed to that trend.  Examples of
contributing factors may include utilization, system health, and component performance.
In the following analyses, we separate our 11,986 performance observations into sets of observations that all ran on the same test platform (as described in Table \ref{tab:platform-descriptions}) to characterize the factors that contribute to time-dependent performance variation across different file systems and architectures.

\subsection{Correlative analysis} \label{sec:results/correlate-mira}

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{mira-correlation-region}
    \vspace{-.35in}
    \caption{Correlation between performance and $CF_{bw}$ in a transition between divergence regions on \mira for all I/O motifs combined.
    (a) shows a region automatically detected using the centroid method, and (b) shows the correlation between performance and $CF_{bw}$ in that region.
    Correlation coefficient is $0.507$ and p-value is ${8.71 \times 10^{-9}}$; dashed line in (b) is a linear fit with slope $0.503$ drawn for visual aid.}
    \label{fig:mira-correlation-region}
    % source: sc18_region-correlation.ipynb
\end{figure}

% Bandwidth, IOPS, and metadata contention are confidently correlated with I/O performance at both long. 

We begin our correlative analysis by partitioning a year-long data set into
divergence regions using the method described in
Section~\ref{sec:features/partitioning}.  Using observations from \mira
\mirafsone as an example, we set $w_{short} = 2 \textup{weeks}$ and $w_{long}
= 7 \textup{ weeks}$ to identify divergence regions, and then discard any
regions with fewer than three data points.  Regions with few data points are discarded
for two reasons: (a) intuitively, very short divergence regions occur
when $\textup{SMA}_{short} \approx \textup{SMA}_{long}$ and there is
minimal long-term variation, and (b) statistically, it is impossible
to assert the statistical significance of a correlation with fewer than
three data points.  This yields 32 divergence regions.

We can then apply Pearson correlation to the feature vectors within
each divergence region to identify the factors that contributed to its
performance trend.  The result of this process is a new feature vector
for each divergence region which contains the correlation coefficients
and p-values between the fraction of peak performance and every other
attribute across all observations in that region.  We also use the Pearson
Correlation to further downselect the total set of regions to those 
with extremely high significance
($\textup{p-value} < {1.0 \times 10^{-5}}$).  This threshold yields a total
of nine relevant divergence regions on \mira \mirafsone.  Each of these nine
regions exhibits at least moderate correlation ($R$) ranging from 0.507 to
0.884 with the bandwidth coverage factor feature.
\TODO{In the above, would it be helpful (and/or possible without much work)
to have a figure that highlights the 9 regions we found over the full year
for Mira?  That's going to be a question in the minds of readers, of what
these regions look like.}

%\TODO{Is the discarding of regions with less than 3 data points really necessary, or would they have been discarded by the p-value threshold anyway?}
% Glenn: I asked myself the same Q.  It turns out that a correlation between exactly two data points produces a perfect p-value of 0.0 because you can draw a straight line through those two points.  This passes the significance filter but royally messes up the results.

Figure~\ref{fig:mira-correlation-region} illustrates the correlation
between bandwidth coverage factor and fraction peak performance for a
divergence region in November 2017 on \mira \mirafsone.  This example
exhibits the lowest correlation ($R = 0.507$) with bandwidth coverage
factor of any of the selected regions, and the scatter plot of performance
results shows why.  In this case there is a cluster of poorly performing
jobs (${0.2 < \textup{fraction peak perf} < 0.4}$) that ran with a
relatively high bandwidth coverage factor.  This region is also the
single largest divergence region observed on \mira with over 20\%
difference between $\textup{SMA}_{short}$ and $\textup{SMA}_{long}$
observed during this time.  This divergence region example highlights the importance of not
just identifying regions and calculating correlations, but identifying cases
in which the analysis indicates the presence of an unknown factor that is
not adequately captured by the instrumentation framework.

Despite the unusual region shown in Fig. \ref{fig:mira-correlation-region},
however, these data indicate that the time-dependent performance divergences observed on \mira show either moderate or strong coincidence with high degrees of bandwidth contention.
Additionally, this correlation with performance degradation occurs across
\emph{all} I/O motifs (similar to Figure \ref{fig:regions-heatmap}a) which
distinguishes it from the motif-specific case discussed in Section
\ref{sec:features/timedependent}.  

When the same Pearson correlation is calculated across the entire collection
of \mira \mirafsone data, the correlation with bandwidth coverage factor
yields an overall result of $R = 0.483$ and $\textup{p-value} =
2.25\times 10^{-88}$.  \textbf{By focusing our analysis on divergence regions we not only
draw attention to the most intuitively important trends in the data, but we
also identify statistically stronger correlations that can be found via
global analysis.}
\TODO{We could use a plot for comparison if we have a lot of free time to
kill.  I don't think it is critical though.}

\subsection{Survey of divergence regions} \label{sec:results/correlate-all}

% - Bandwidth, IOPS, and metadata contention are confidently correlated with I/O performance at both long. 



\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{trend-correlations}
    \vspace{-.35in}
    \caption{Correlations discovered between fraction peak performance and all other attributes measured during job execution.
    Each circle represents the correlation coefficient over a single trend region, and its diameter is proportional to $-\log_{10}(\textup{p-value}$).
    CF denotes coverage factor.}
    \label{fig:trend-correlations}
    % source: sc18_centroids-allsystems.ipynb
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{cscratch-bimodal-fsaveosscpu}
    \vspace{-.35in}
    \caption{Regions of negative correlation (red) and positive correlation (blue) between fraction peak performance and data server load during trend regions identified on \cori.
    SMAs for \cori's HACC write workload also shown to illustrate the coincidence of a long-term performance issue with the direction of correlation.}
    \label{fig:cscratch-bimodal-fsaveosscpu}
    % source: sc18_centroids-allsystems.ipynb
\end{figure}

Next we apply the same correlation analysis to the other machines in our study, again only keeping correlations with an extremely high significance ($\textup{p-value} < {1.0 \times 10^{-5}}$), and the results of this analysis are shown in Fig. \ref{fig:trend-correlations}.
As was found with Mira in Section \ref{sec:results/correlate-mira}, there is moderate to strong correlation between I/O performance and the bandwidth coverage factor on the Lustre file systems of \cori, and \edison.
Although bandwidth contention resulting in performance loss is intuitive at the scale of a single performance transient, the fact that these correlations were found over longer-term divergence regions indicates that \textbf{bandwidth contention from \emph{sustained workloads} often accompany sustained performance losses}.
This is particularly relevant to the increasing fraction of experimental and observational data that is being processed on modern HPC platforms; as the volume of data being continually streamed from large-scale scientific instruments increases, the effects of sustained bandwidth contention are likely to become increasingly prominent.

% - The sign and magnitude of the correlations changes over time.

Another noteworthy feature that this method reveals is the bimodality of correlation between performance and the CPU load of the file system data servers (``Data Server Load'' in Fig. \ref{fig:trend-correlations}) on the Lustre-based test platforms.
A time-resolved view of the regions where performance correlates with data server CPU loads (Fig. \ref{fig:cscratch-bimodal-fsaveosscpu}) reveals that the bimodality of the correlation matches the biomodality observed in the HACC write workload on the affected storage systems.
During the long-term performance regression discussed in Section \ref{sec:features/timedependent}, high CPU load on the Lustre OSSs coincided with low performance of the I/O performance probes.
As soon as performance was restored on August 10, the relationship reversed, and high CPU load was observed favorably with respect to performance.
The positive correlation between performance and CPU load is consistent with the data servers using CPUs primarily to service incoming I/O requests, whereas the negative correlation indicates that another CPU load (as may be caused by an algorithmic bug) was present and competed with the data servers' ability to use CPU to service those same requests.
From this, we conclude that not only does baseline I/O performance vary with time, but \textbf{the nature and magnitude of how different attributes correlate with I/O performance also change over time}.
Had this correlation analysis been performed without partitioning over divergence regions, the regions of positive and negative correlation would have obfuscated each other in the net result.

The remaining two attributes that were found to correlate with performance
are file system fullness and \texttt{open(2)} coverage factor (a
measure of metadata resource contention).  The two instances of file
system fullness correlating negatively with performance are clear
and can be corroborated with independent observations from facilities
staff.  These two regions encapsulate periods when their respective
file systems approached 90\% full for a period of several days.
The observed loss of performance is consistent with Lustre's known
susceptibility to significant performance degradation as OSTs approach
90\% fullness~\cite{oral2014best,Lockwood2017}.  The correlation with
\texttt{open(2)} coverage factor is intuitive because this metric acts as a
proxy for metadata contention.  
One of these divergence regions was found to overlap with an unusually long-running and extensive, multi-day purge of the \edison \scratchtwo file system.
However, it is unclear at this time what caused the other correlated divergence region.
% The above statement is not very satisfying, but we don't have time to do more detailed analysis.  The regions are:
% coverage_factor_opens 2017-03-29 19:11:51 - 2017-04-15 19:13:37 (71 points)
% coverage_factor_opens 2017-11-26 18:26:41 - 2017-12-13 20:03:11 (141 points)


\subsection{Transient performance loss} \label{sec:results/shortterm}

\begin{figure}

    \centering
    \includegraphics[width=1.0\columnwidth]{contributors-bad-by-system-grey}
    \vspace{-.35in}
    \caption{Attributes that correlated with poor I/O performance across all file systems and benchmarks tested normalized to the number of probes during which each attribute was measured.
    \mira was the only system for which IOPS coverage factor was measured.
    Greyed bars are attributes whose aggregate contributions were not statistically significant.
    Percentages do not add up to 100\% because multiple attributes are often classified as contributors for a single anomalous job.
    }
    \label{fig:contributors-bad-by-system}
    % source: sc18_umamify-all.ipynb
\end{figure}

In addition to characterizing long-term performance issues, it is also advantageous to determine the reasons why I/O performance is severely degraded for one and only one day in an otherwise unremarkable period of time.
Such performance losses, exemplified as isolated dark blocks in Fig. \ref{fig:regions-heatmap}, may only be observed in one of the I/O motifs probed on that day, suggesting a very short-lived issue that disappeared over the course of one or two of the eight daily performance probes.
The lack of a consistent performance trend surrounding these transients makes them difficult to correlate with other metrics as was done in the previous section, necessitating a different approach to characterizing them.

To address this need to characterize isolated performance anomalies, we apply the same strategy of partitioning performance observations into divergence regions and then performing statistical analysis within each region.
To identify individual performance anomalies in a divergence region and classify the factors which contributed to them, we apply the following method:

\begin{enumerate}[leftmargin=*]

\item We examine the feature vector for every observation in the divergence region, and for each attribute $a$, determine the observation where that attribute's measured value was at its lowest, $\min(a)$.

\item We then define the \emph{anomalous observation} for the divergence region as the observation whose feature vector contains $\min(a)$ for fraction peak performance.
Because divergence regions contain observations of similarly divergent performance by definition, this anomalous observation is truly anomalous--not only is it in a region of performance divergence, but it is also divergent from that divergence.
\TODO{Is there a better way to phrase this than divergent from that
divergence?}

\item Finally, we determine which other $\min(a)$ values also fell in this anomalous observation's feature vector and classify those attributes $a$ as contributors to the anomalous observation's anomalous performance.

\end{enumerate}

Qualitatively, this process formalizes the conjecture that if I/O performance is anomalous on a specific day, the other measured attributes which were also anomalous at that time contributed to that anomalous performance.
Quantitatively, this simple classification scheme allows us to identify correlations and quantify the statistical significance of of each classification for individual anomalous observations.
To assess the statistical significance of each classification, we calculate the p-value of each classification in a divergence region of $N$ observations as $\textup{p-value} = \frac{1}{N}$; intuitively, this is the probability of a positive classification being applied to a random variable.

The result of this process is zero or more attributes being positively classified as contributors to anomalous performance.
For example, if the lowest values of the bandwidth coverage factor and IOPS coverage factor attributes occur in the same feature vector as the lowest value for performance, we classify both bandwidth and IOPS coverage factors as contributors to that anomalous observation's behavior.

To apply this method to our performance probes, we first group observations into sets of data by the test platform on which they ran as was done in Section \ref{sec:results/correlate-mira}.
In addition, these sets are further subdivided according to I/O motif and read/write character of the performance probe to classify anomalies at full temporal resolution and motif-level granularity.
The net result are $5 \times 4 \times 2 = 40$ sets of data, each representing a unique combination of test platform ($5 \times$, as listed in Table \ref{tab:platform-descriptions}), I/O motif ($4 \times$, per Table \ref{tab:benchmark-motifs}), and whether the probe was reading or writing ($2 \times$).
Schematically, each horizontal row in Figure \ref{fig:summary-heatmap} represents a single set.
For each set of observations, SMAs are calculated, crossover points are defined, and the set is partitioned into a set of divergence regions.

The result of this partitioning are 1,146 divergence regions across 40 sets of observations, each with one anomalous observation.
For each divergence region, we then apply the classification procedure described and generate positive classifications of attributes that affected performance and the p-values of those classifications.
We discard all statistically insignificant classifications ($\textup{p-value} > 0.10$) to eliminate divergence regions that are too small to make any meaningful classifications, positive or negative.
The final product of this analysis are 490 anomalous observations, 410 of which have at least one attribute positively classified as a contributor.

As described in Section \ref{sec:methods/tokio}, not every observation's feature vector has every feature defined.
So as not to bias our results away from those attributes that were only measured for a subset of observations (e.g., IOPS coverage factor was only collected on \mira), we express the frequency of each attribute as the ratio of observations where it was defined \emph{and} classified to the number of observations where it was defined.
The result of this aggregated summary is shown in Figure \ref{fig:contributors-bad-by-system}.

As with the longer-term performance divergence characterized in Section \ref{sec:results/correlate-all}, high contention for bandwidth is found to also cause short-term performance transients.
However, contention for IOPS is also positively classified in a significant fraction of anomalous observations despite it not arising as a factor in longer-term performance divergence.
This contrast demonstrates that IOPS contention only becomes a statistically significant attribute related to performance degradation in short-term transients, and there are no sustained periods of high IOPS on \mirafsone.
This also distinguishes this utility of this transient classification from
the longer-term correlative analysis.

We also observed correlations with metadata coverage factor and CPU load on
data servers during anamolous jobs to a less significant degree.  This is
consistent with the correlation coefficients shown in
Figure~\ref{fig:trend-correlations}.
Unlike the correlative analysis, however, metadata contention was found to impact individual jobs on every test platform and occurred at much higher frequency.
These findings indicate that metadata contention is much more likely to coincide with transient performance loss than a long-term and sustained divergence of performance.

In general, the contrast in correlations between transient events and
long-term trends shows that \textbf{transient
problems correlate with different attributes (such as IOPS and
metadata contention) those found in analysis of long-term problems (such
as sustained bandwidth contention).} These are distinct conditions that
require unique investigation techniques.

Metadata server CPU load, file system fullness, and maximum job radius were also positively classified as contributors to anomalous performance in a small fraction of anomalous observations.
To ascertain whether the rate at which these three attributes were classified is statistically significant, we can assign p-values to aggregate fraction of anomalous observations where each attribute was positively classified in Figure \ref{fig:contributors-bad-by-system}.
We leverage the fact that we have a raw count of classifications (positive and negative) for each anomalous observation and p-values for each divergence region to apply a one-tailed binomial test
   \footnote{For the binomial test, we assume a constant probability of positively classifying a random variable in each divergence region.
   Given our dataset was filtered to only include regions with $\textup{p-value} < 0.10$, we used 0.10 as the most conservative value for this binomial test probability.}
and calculate the probability of observing the stated distributions in random variables.

The result of this significance analysis is the finding that the rates at which these three low-frequency attributes (metadata server load, file system fullness, and maximum job radius) were classified as contributors to anomalous performance are statistically insignificant.
Whereas the leftmost five attributes in Fig. \ref{fig:contributors-bad-by-system} all have p-values of ${5 \times 10^{-5}}$ or lower, the insignificant metrics' are 0.15 or higher.
Qualitatively, these negative findings are not unreasonable; for example, file system fullness is most often a degenerative, long-term health problem as was identified in Section \ref{sec:results/correlate-all} and prior work~\cite{oral2014best,Lockwood2017}.

The classification process and analysis described here demonstrates that it is possible to apply statistical analysis to fine-grained divergence regions and still obtain statistically significant insight into the causes of transient performance degradation.
While we chose a very simple classification criteria based on $\min{a}$, this process can be applied using any classification methodology that identifies relationships between feature vectors in a divergence region and quantifies the associated statistical significance.
For example, classifying attributes based on quartiles rather than minima would be feasible with more coarse-grained divergence regions to offset the decrease in significance inherent in the selection criteria.

\subsection {Discussion}
\label{sec:results/discussion}

\TODO{This is where interesting dribs and drabs are winding up.  Do we carve out a separate section about this stuff, or just drop it?}

Our choice of averaging $\textup{SMA}_{short}$ over ${-0.5w_{short} <= t < +0.5w_{short}}$ makes it insensitive to choice of $w_{short}$.
For the analysis in Section \ref{sec:features/timedependent} changing $\textup{SMA}_{short}$ from $\textup{w}_{14}$ to both $\textup{w}_{7}$ and $\textup{w}_{28}$ resulted in no change to the dates bounding the 139-day divergence region.
The principal effect of changing $\textup{w}_{short}$ is the number of short regions that arise from the higher-frequency oscillations of $\textup{SMA}_{short}$ around $\textup{SMA}_{long}$.

\TODO{This is where a discussion about performance models having to account for long-term changes to baseline performance should go.  That doesn't seem to fit well into the rest of the narrative though.}

We found the exact choice of $w_{short}$ and $w_{long}$ to be somewhat arbitrary; adjusting these values by as much as $\pm 50\%$ did not affect the identification of the most significant events presented here.
In addition, a specific choice of $w$ does not preclude analyzing events longer or shorter than $w$, and we demonstrate methods to address this in Sections XYZ.

Although SMA crossover points are also applied in market analysis for detecting trends in prices and predicting when to buy or sell an asset based on the direction of the crossover~\cite{brock1992simple}.
However, the predictive value of SMA crossovers is a source of controversy even in the financial community, and thus, we focus here solely on using them as tools for detecting trends and partitioning divergent regions in historical datasets.

Financial market analysis techniques can be used to detect underlying trends within noisy I/O performance data.
