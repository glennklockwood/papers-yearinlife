\section{Introduction}

I/O performance variation has been studied extensively, and a variety of conditions have been identified as contributing factors to poor I/O performance.  Most studies have focused on enumerating the sources of I/O performance loss at a single point in time, assuming that performance loss is a transient effect due to contention from other jobs.  However, recent work~\cite{Lockwood2017} has shown that performance variation can occur over periods of days as a result of systematic, longer-term conditions of a storage system.
%
Overall performance (and thus scientific productivity) can be improved for a
wide range of users if these deviations can be identified and attributed
quickly in production. Additionally, the determination of the reasons for the performance slowdowns can be fed into the design of future file systems. 

%Such systematic, long-term I/O performance variation remains much less well understood despite its relevance to scientific campaigns that may experience uneven throughput or resource consumption over the course of months or years.  In this work, we differentiate \emph{long-term performance variation} from \emph{short-term performance variation} and characterize the factors that contribute to long-term performance variation on a diverse range of large-scale production parallel storage systems.

A number of recent efforts~\cite{Lockwood2017,Vazhkudai2017guide,Agelastos2014ldms,Kunkel2014siox} have advanced the
state of the art in scalable data collection, making it possible to observe
production systems at unprecedented scales.  It remains an open problem,
however, how to best \emph{interpret} this data to quickly for maximum production
impact, and what types of instrumentation have the greatest return on
investment.
Aligning these broad and diverse datasets and contextualizing and interpreting their contents require both a deep understanding of I/O performance and a broad understanding of statistical analysis techniques.
\TODO{Add sentences on why the above questions are still open and/or what the challenges are in solving them to emphasize the importance of this work. -- Suren
\\
% "large datasets, diverse datasets, lack of common analysis techniques"
% "correlating, aligning, interpreting data from multiple data sources"
gkl: gooder now?
}

We investigate this issue by studying performance data collected
from large parallel file systems at two leadership-class HPC centers
over the course of a year of production use.  In addition to passive
instrumentation, such as system monitoring and application profiling,
we also use active probing of I/O performance to record user-perceived
performance over time.  The breadth of the data set enables investigation
over multiple time scales, ranging from days to months, to identify trends in
both absolute performance and variability.  The depth of the data set
enables correlation analysis to identify subtle relationships between
performance and a variety of system-wide metrics.

%We also develop a methodology for identifying inflection points in
%long-term user-perceived performance due to factors beyond their control,
%such as hardware failures, software changes, capacity constraints, or
%contention.  These trends are easily overlooked or misattributed given the
%complexity and inherent variability of the I/O system itself, but can have a
%significant long term impact on efficiency.  Once an inflection point has
%been found, we investigate methods for guiding administrators to likely
%contributing factors if not outright identifying definitive root causes for
%anomalous performance.

%By integrating holistic I/O monitoring throughout the
%year-long experiment, we then quantitatively show
%that common sources
%of short-term variation, such as bandwidth or metadata contention,
%play less significant role in long-term performance variation.
%Given this multiscale nature of I/O performance variation, we conclude that the probabilistic effects of transient interference \emph{and} the autocorrelative effects of long-term variation are required to capture the full picture of performance variation on parallel storage systems.

The primary contributions of this work are as follows: 
\begin{itemize}[leftmargin=*]
\item We have collected, and will make publicly available, an unprecedented year long, multi-facility
I/O performance data set. It includes the results of daily I/O performance
probes and their associated telemetry data.
\item We show that baseline performance and variability change over time.
Factors such as system software updates and sustained I/O-intensive workloads contribute to long term variations.
\item We show that the nature of correlations between I/O performance and system metrics also change over
time. For example, we demonstrate that high CPU load can correlate with favorable performance under healthy file system conditions, and it can coincide with unfavorable performance when non-I/O workloads are impacting storage servers.
\item We show that contention for bandwidth, IOPS, and metadata resources can be confidently determined to be the sources of transient I/O performance problems.
\item In the course of our study, we develop methods for identifying
underlying trends within noisy telemetry data, and show how analysis focused
on these trends improves the quality of the analysis results.
\end{itemize}
Overall we show that a great deal of information about I/O performance variation can be quantified using holistic I/O analysis and simple statistical methods.
This paper is organized as follows: \S~\ref{sec:related} contains related work, \S~\ref{sec:methods} describes the data collection framework, the benchmarks used, and the HPC platforms studied, \S~\ref{sec:features} describes our methodology for determining regions of similar I/O performance, \S~\ref{sec:results} contains the results of our statistical analysis of the data, and \S~\ref{sec:conclusions} concludes. 

